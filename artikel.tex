\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[UKenglish]{babel}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\usepackage{tabularx}
\usepackage{booktabs}

\usepackage{mathtools}
\usepackage{enumitem}
\setenumerate[0]{label=(\arabic*)}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output


% Kan Danny godt lide!
\usepackage[autostyle]{csquotes}
\usepackage{kpfonts}
\usepackage{inconsolata}
\linespread{1.06}

\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{%
	pdftitle=Reinforcement learning for mathematical applications,
	pdfauthor={Andreas Bøgh Poulsen},
	colorlinks,
	linkcolor={red!50!black},
	citecolor={red!50!black},
	urlcolor={red!50!black},
	bookmarksnumbered=true
}

\usepackage[ntheorem]{mdframed}
\usepackage[amsmath,thmmarks,hyperref]{ntheorem}
\usepackage[capitalize]{cleveref}

% Frame for theorems
\definecolor{shadecolor}{gray}{0.93}
\definecolor{rulecolor}{gray}{0.4}
\mdfdefinestyle{thmframed}{%
	%usetwoside=false, % For use with memoir twoside
	skipabove=0.5em plus 0.4em minus 0.2em,
	skipbelow=0.5em plus 0.4em minus 0.2em,
	leftmargin=-7pt, rightmargin=-7pt, innerleftmargin=6pt,
	innerrightmargin=6pt, innertopmargin=6pt, innerbottommargin=3pt,
	linewidth=1pt, linecolor=rulecolor, backgroundcolor=shadecolor,
	splittopskip=1.2em minus 0.2em,
	splitbottomskip=0.5em plus 0.2em minus 0.1em,
}

% New theorem style with a dot
\makeatletter
\newtheoremstyle{changedot}%
  {\item[\hskip\labelsep \theorem@headerfont ##2~~$\cdot$~~##1\theorem@separator]}%
  {\item[\hskip\labelsep \theorem@headerfont ##2~~$\cdot$~~##1\ (##3)\theorem@separator]}

\newtheoremstyle{changedotbreak}%
  {\item\hbox to \textwidth{\theorem@headerfont ##2~~$\cdot$~~##1\theorem@separator\hfill}}%
  {\item\hbox to \textwidth{\theorem@headerfont ##2~~$\cdot$~~##1\
      (##3)\theorem@separator\hfill}}
\makeatother

\theoremstyle{changedot}
\theoremseparator{.}
\newmdtheoremenv[style=thmframed]{theorem}{Theorem}[section]
\newmdtheoremenv[style=thmframed]{proposition}[theorem]{Proposition}
\newmdtheoremenv[style=thmframed]{lemma}[theorem]{Lemma}
\newmdtheoremenv[style=thmframed]{corollary}[theorem]{Corollary}

\theorembodyfont{\normalfont}
%\theoremsymbol{\ensuremath{\triangle}}
\newmdtheoremenv[style=thmframed]{definition}[theorem]{Definition}

\theoremstyle{changedotbreak}
\newmdtheoremenv[style=thmframed]{definitionbreak}[theorem]{Definition}

\theoremstyle{nonumberplain}
\theoremheaderfont{\normalfont\itshape}
\theorembodyfont{\normalfont}
\theoremsymbol{\ensuremath{\square}}
\newtheorem{proof}{Proof}
\newtheorem{remark}{Remark}

\Crefname{theorem}{Theorem}{Theorems}
\Crefname{proposition}{Proposition}{Propositions}
\Crefname{lemma}{Lemma}{Lemmata}
\Crefname{corollary}{Corollary}{Corollaries}
\Crefname{definition}{Definition}{Definitions}

\crefformat{equation}{(#2#1#3)}

% / Kan Danny godt lide

% Kan Andreas godt lide
\setlength{\parindent}{1em}
\setlength{\parskip}{0.7em}


\title{Reinforcement learning for mathematical applications}
\author{
    Andreas Bøgh Poulsen\\
    201805425
}
% \date{28th June 2020}


\usepackage{csquotes}
\usepackage[backend=biber, style=alphabetic, maxcitenames=1]{biblatex}
\addbibresource{references.bib}

\usepackage{microtype}

\usepackage{graphicx}




\newcommand{\ro}[1]{^{\setminus #1}}
\newcommand{\rr}  {^{\setminus}}

\DeclarePairedDelimiter{\tuple}{\langle}{\rangle}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calV}{\mathcal{V}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calG}{\mathcal{G}}

\newcommand{\true}{\textsc{true}}
\newcommand{\false}{\textsc{false}}

\DeclareMathOperator*{\smallbigcup}{\textstyle\bigcup}
\DeclareMathOperator*{\bigunion}{\mathchoice
	{\smallbigcup}%
	{\bigcup}%
	{\bigcup}%
	{\bigcup}%
}
\DeclareMathOperator*{\smallbigcap}{\textstyle\bigcap}
\DeclareMathOperator*{\bigintersect}{\mathchoice
	{\smallbigcap}%
	{\bigcup}%
	{\bigcup}%
	{\bigcup}%
}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\newcommand{\setN}{\mathbb{N}}
\newcommand{\setR}{\mathbb{R}}
\newcommand{\card}[1]{\abs{#1}}

\let\phi\varphi
\frenchspacing

% Partial functions
% From https://tex.stackexchange.com/questions/47142/how-to-tex-an-arrow-with-vertical-stroke

\makeatletter
\newcommand{\pto}{}% just for safety
\newcommand{\pgets}{}% just for safety

\DeclareRobustCommand{\pto}{\mathrel{\mathpalette\p@to@gets\to}}
\DeclareRobustCommand{\pgets}{\mathrel{\mathpalette\p@to@gets\gets}}

\newcommand{\p@to@gets}[2]{%
  \ooalign{\hidewidth$\m@th#1\mapstochar\mkern5mu$\hidewidth\cr$\m@th#1\to$\cr}%
}
\makeatother

\newcommand{\id}{\mathrm{id}}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\anglemap}{\langle}{\rangle_{\infty}}

\newcommand{\nextk}{\mathrm{next}}
\newcommand{\enc}{\mathrm{enc}}

\newcommand{\binlit}[1]{\mathbf{#1}}
\newcommand{\bin}{\mathrm{bin}}
\newcommand{\val}{\mathrm{val}}
\newcommand{\Seq}{\mathrm{Seq}}
\newcommand{\length}[1]{\abs{#1}}

\DeclarePairedDelimiter{\gpath}{\langle}{\rangle}


\begin{document}

\maketitle

\section{Preliminaries}

\subsection{Abstract framework of deep learning}
In abstract, modern machine learning is about fitting a universal approximator
to some given data. In recent instances of it, this universal approximator would
be a deep neural network, possibly with some variations like convolution,
recurrence or attention. Using differentiation of a neural network and gradient
descent we can minimize the error between the predictions of the network and
some training data.

I will not spend much time on this part and take the training of a neural
network for granted. The important point is that we can minimize any error
function, not just the difference between a prediction and some known data.

This is an important insight. If we can check if the network is producing right
result after the fact, we don't have to know the facit beforehand. This is good
because we can use the network to discover solutions without having to actually
solve the problem ourselves. For example, if we wanted to learn to find a
divisor of a number, it's a lot easier to check a solution than finding a factor
ourselves, so this is a good problem for machine learning. We can just set the
error to zero if we found a factor and 1 if we didn't. On the other hand, it's
difficult to check wether a number is a prime number or not, so this probably
wouldn't be a good problem to solve with machine learning.

\subsection{Reinforcement-learning}
Let's build a bit more on the insight from before: we can minimize any error
function. We can use this to extend the domain of our learning. One interesting
case is learning to interact with an environment. This could be a robot
interacting with a physical environment or a mathematician trying to prove a
theorem. Both cases can be thought of as an agent doing an action, seeing how it
worked out and taking a new action. Let's model such an interaction formally.

\begin{definition}
  Fix two non-empty sets $S, A$ and call $S$ the \emph{state-space} and $A$ the \emph{action-space}. Then, an \emph{environment} is a tuple $(S, A, P, R, \epsilon, t)$ where

  \begin{itemize}
    \item $P : S \to 2^{A}$ gives the permissible actions of a given state.
    \item $R : s\in S \times P(s) \to \mathbb R$ is called the \emph{reward-function}.
    \item $\epsilon : s\in S \times P(s) \to S$ is called the \emph{update-function}. This might a probabilistic function i.e. non-deterministic.
    \item $t : S \to \{True, False\}$ determines if a state is a \emph{terminal} state.
  \end{itemize}

  Given an environment, an \emph{agent} is a function $M : s\in S \times P(s) \to A$.
\end{definition}

In most cases all actions are always permissible, i.e. $P(s) = A$ for all $s \in S$. The intuition is the following setup:

\begin{enumerate}
  \item An agent observes an environment in state $s$ and decides to take some action $a \in P(s)$.
  \item A reward of $R(s, a)$ is given to the agent to tell it wether the action was good or not.
  \item The environment is updated to $(S, s'=\epsilon(s), A, P, R, \epsilon, t)$.
    \item If $t(s') = True$ the interaction is done. If not, repeat from step 1.
\end{enumerate}

Of course the idea is that the agent would learn to take actions in such a way that the reward is maximized.

In order to facilitate learning, we need to repeat the process described above a number of times, so let's fix some naming and notations for that:

\begin{definition}
  Given an \emph{environment} $(S, A, P, Q, \epsilon, t)$, an \emph{agent} $M : s \in S \times P(s) \to A$ and an initial state $s_{0} \in S$ a \emph{Markov decision process} is an iterative process of states and actions, defined recursively as
  \begin{align}
    s_{0} &= s_{0} \\
    s_{t+1} &= \epsilon(s_{t}, a_{t}) \\
    \text{where } a_{t} &= M(s_{t})
  \end{align}

  A \emph{trajectory} is a record of a Markov Decision Process, formally a tuple \[(s_{t}, a_{t}, r_{t}, t_{t})\] where $s_{t}$ and $a_{t}$ are described above and $r_{t} = R(s_{t}, a_{t})$, $t_{t} = t(s_{t})$.
\end{definition}


\section{Learning strategies}
The abstract framework of Markov Decision Processes is a rather simple idea. And indeed, that is not where the magic of reinforcement learning is. None of the above even hinted at how any learning would occur, we have only simulated an environment and doled our rewards. It shouldn't be surprising that there are many strategies when it comes to learning. We'll outline few of them here, as well as few ``tricks'' that can be used to improve performance.

In order to illustrate these ideas, we'll use tic-tac-toe as example. I hope that this is a familiar game to everyone. I will focus on the pen-and-paper version, where a piece is placed and never moved, but the ideas carry over to other variations of the game.

\subsection{Value-estimation}
Think about how you would play tic-tac-toe yourself. At least when I play, I have the following heuristc: a situation where my opponent has an opportunity to win is bad. A situation where I have an opportunity to win is good. A situation where I have two opportunities to win is even better and similarly worse if my opponent have two opportunities. Also, a situation that might lead me a very good situation is pretty good, for example if I'm cross here and it's my turn:

\begin{tabular}{c|c|c}
    &   & X \\ \hline
    & O &   \\ \hline
  X &   & O
\end{tabular}

All these considerations come together, and I quickly give each situation a score and choose the action that is most likely to lead me to a good situation, here placing a piece in the upper left corner.

This might lead us to a good learning strategy. If we can learn how valuable each possible situation is, we're in pretty good shape. If we can then estimate how likely an action is to lead us to each situation, we can choose the action that maximizes value.

This idea can be treated using the Bellman equation. We'll consider the deterministic case first, where the Bellman equation is simply
\[V(s_{0}) = \max_{a_{0}} \{R(s_{0}, a_{0}) + V(x_{1}) \} \text{, where } x_{1} = \epsilon(s_{0}, a_{0})\]
The idea is simple, the value of a given state is the value of the best next state plus the reward given by progressing to that state. 

\colorbox{red}{TODO: extend this to stochastic case}

\subsection{Q-learning}
In value-estimation we determine how valuable a given state is. However, that is only a partial picture. We got into some trouble as soon as we can't predict the next state from any particular action. Another approach is Q-learning, which a somewhat different perspective.

Instead of assigning a value to each state, we estimate the expected reward coming from a particular \emph{action}. That is
\[Q(s, a) = \mathbb{E}[\sum_{i=t}^{T} R(s_{i}, a_{i}) | s_{t}=s, a_{t}=a, \text{agent } m]\].
% \colorbox{red}{TODO: Find the right formula} I think it's correct now

Q-learning is one of the first reinforcement learning algorithms that achieved break-through performance. One the first examples was the program TD-gammon, a program that learned to play backgammon. TD-gammon was novel because it exploited no domain knowledge of backgammon at all. It simply employed Q-learning and yet was able to match the performance of the best backgammon-bots of the time. These other bots all used extensive expert knowledge of the game so it was remarkable that Q-learning could learn to play as well as them. The first iteration of TD-gammon used simply naive Q-learning. Later iterations achieved even better performance by adding Monte Carlo Tree search (which we'll cover later) to the algorithm. While this did improve performance it's remarkable that high performance can be achieved without anything but a simple learning algorithm.

In small cases, we can keep the Q-function in a simple lookup table. So, given a state and an action, we can do a lookup to find our estimation of the future reward given that action. In this case, the learning algorithm is simply:
\[Q(s_{t}, a_{s}) \leftarrow \begin{cases} (1 - \alpha)Q(s_{t}, a_{t}) + \alpha (r_{t} + \max_{a} Q(s_{t+1}, a)) & \text{if  $s_{t}$ is non-terminal} \\ (1 - \alpha) Q(s_{t}, a_{t}) + \alpha r_{t} & \text{if $s_{t}$ is terminal} \end{cases}\]
where $\alpha$ is the learning rate. The intuition here is that we now know what reward we got by taking action $a_{t}$. Therefor we can update our estimate to be a combination of our old estimate and an updated estimate based on the observed reward plus our estimate for the rest of the process.

Let's see how that would work in the our tic-tac-toe example. Whenever the agent makes a move we give it a reward of $+1$ if it won by that move and $-1$ if the opponent won on their following move. This is typical for reinforcement learning; we give a reward when a task is completed and expect our agent to learn how to perform the entire task. It sounds a bit unreasonable, doesn't it? It's obvious that the action immediately before a reward was a good action, but what about the action before that? It might have been good, or it might have been bad even though we won the action after. The cause-and-effect chain gets weaker every link. In practice, it works anyway.

Consider what happens after we won a game. The agent updates that the last action was a good one, so $Q(s_{T}, a_{T})$ where $T$ is the last step of the trajectory, grows. What about the rest of the actions? Well, they are not immediately updated. However, the next time the agent faces a situation just before the winning move $a_{T}$, the estimated future rewards from that situation grows since we know we can win after it. Similarly, next time the agent faces a situation just before the one we just considered, the reward is propagated backwards.

The backwards propagation of rewards allows the agent to learn complex strategies. Even though any one trajectory may contain good and bad moves, on average trajectories that leads to a win will share good moves and trajectories leading to a loss wil share bad moves. It may seem a bit wishful to think at they will cancel each other out, but Watkins and Dayan proved convergence to optimal strategy.

\subsubsection{Deep Q-learning}
A major challenge for Q-learning is the table of states and actions. As our problem grows in complexity, this table becomes large very quickly. To remedy this, modern Q-learning uses function approximation to learn a single $Q : state \times action \to reward$. The DeepMind team used in 2015 a convolutional neaural network to learn the Q-function and succeeded in learning to play a variety of 49 different Atari games.

In this setting, we need to adapt the learning formula given above. In that case we updated our estimated Q-function to be a linear combination of the previous estimate and our new knowledge. This can be viewed as a simple form of gradient descent towards the observed data.

To do gradient descent on a neural network, we need a loss function $L$ that is the difference between our current estimate and the observed data. It looks like this:
\[L_{t} = \begin{cases}
    \left( r + max_{a} Q(s_{t+1}, a) - Q(s_{t}, a_{t}) \right)^{2} & \text{if $s_{t}$ is non-terminal} \\
    (r - Q(s_{t}, a_{t}))^{2} & \text{if $s_{t}$ is terminal}
  \end{cases}\]

By computing gradients on this function we can update our neural network (or other function approximator) towards the updated values.

\subsubsection{Experience replay}
Normal Q-learning is a temporal-difference learning method, which means it only learns from the timestep it just took. Experience replay keeps track of previous experiences and uses them when doing an update. To do this, we keep a trajectory of the currect episode, sample some random times from it and perform gradient descent on every remembered timestep simultaneously. Specifically, for a random sample $D = \{t_{i}\} \subset \mathbb N$ of timesteps, we compute the vector
\[G_{t} =  \begin{cases}
    r_{t} + max_{a} Q(s_{t+1}, a) & \text{if $s_{t}$ is non-terminal} \\
    r_{t} & \text{if $s_{t}$ is terminal}
  \end{cases} \]
for every $t \in D$ and let our loss-function be the euclidean distance from $G$ to the vector $H_{t} = Q(s_{t}, a_{t})$ for $t \in D$.

There is no proven reason why experience replay should help learning. DeepMind provides that it reduces variance in the gradients, which would help stabilize the learning. I don't want to risk saying something wrong, so I'll just say that it seems to work and that's a good enough reason to use it.

\subsection{Policy gradient}
Value estimation and Q-learning had the nice property that there was in some sense a concrete function we learned, the value associated with something. Policy gradient is different and exploits the backpropagation algorithm. The idea is that instead of trying to approximate something that we know, compute an error function between our current agent and some observed data, we can just specify that some behaviour was good and have the neural network optimize towards doing that again.

That was vague, so let's do some math. We have an agent with some parameters $\theta$ and a reward function $F(t)$ taking a trajectory $t$ and giving us a total reward for that trajectory. Since we want to optimize $E[F(t) \mid \theta]$ we can do that by taking the gradient wrt. $theta$ i.e. $\nabla_{\theta} E[F(t) \mid \theta]$. This expression is difficult to compute, so let's do some rearranging:

\begin{equation} \label{policy_grad}
\begin{aligned}
  \nabla_{\theta} E[F(t) \mid \theta] &= \nabla_{\theta} \sum_{t}P(t \mid \theta) F(t) \\
                                      &= \sum_{t} F(t) \nabla_{\theta} P(t \mid \theta)  \\
                                      &= \sum_{t} F(t) P(t \mid \theta) \frac{\nabla_{\theta} P(t \mid \theta)}{P(t \mid \theta)} \\
                                      &= \sum_{t} P(t \mid \theta) \nabla_{\theta} \log P(t \mid \theta) && \text{since $\frac{\nabla_{\theta} x}{x} = \nabla_{\theta} \log x$.} \\
                                      &= E[F(t) \nabla_{\theta} \log P(t \mid \theta)]
\end{aligned}
\end{equation}

Now we're in good shape. Since $\log$ is a monotonically growing function we now get, that if $F(t) > 0$ we can increase $E[F(t) \mid \theta]$ by increasing $P(t \mid \theta)$, at least locally. Similarly, if $F(t) < 0$, decreasing $P(T \mid \theta)$ will increase $E[F(t) \mid \theta]$.

\begin{remark}
  The above derivation can be done more generally than we've done in \ref{policy_grad}. By assuming an arbitrary, not necessarily discrete, probility distribution of states, we could replace the sum with an integral for a more general derivation.
\end{remark}

Right, so how do we increase the probability of a trajectory? Intuitively, we just increase the probability of the actions the trajectory consists of, and this turns out to be sufficient. Since each action is taken independently of previous actions, we have \[P(t \mid \theta) = P(s_{0}) \prod_{i=0}^{T-1} \pi_{\theta}(a_{i} \mid s_{i}) P(s_{i+1}, r_{i} \mid s_{i}, a_{i})\] where $pi_{\theta}$ gives the probability distribution of actions that the agent is sampling from. Taking the logarithm and gradient on both sides, we get that \[\nabla_{\theta} \log P(t \mid \theta) = \nabla_{\theta} \sum_{i=0}^{T-1} \log \pi_{\theta}(a_{i} \mid s_{i})\] and thus, by taking this together with \ref{policy_grad} that \[\nabla_{\theta} E[F(t) \mid \theta] = E\left[ F(t) \nabla_{\theta} \sum_{i=0}^{T-1} \log \pi_{\theta} (a_{i} \mid s_{i}) \right]\] and this finally gives us a good way to update our neural network: simply increase $\pi_{\theta} (a_{i} \mid s_{i})$ by following the gradient of the neural network.



Now, this is great if we have direct access to a gradient descent optimizer. However, most modern machine learning frameworks is fixed to the model of minimizing an error function. The solution is very simple; to maximize $F(t) \sum_{i} \log \pi_{\theta} (s_{i} \mid s_{i})$ we can minimize $-F(t) \sum_{i} \log \pi_{\theta} (a_{i} \mid s_{i})$. Thus our loss could be just that.

\subsubsection{Normalization}
There are a number of challenges when doing policy gradient learning in practise. One is that rewards are often very high variance and not balanced. For example, if we only care about achieving a goal, every reward will be positive so every action will be encouraged. The reason things still work in this situation is that actions leading to higher rewards will lead to steeper gradients and be ``encouraged more''. It might give better results if we could normalize our rewards by subtracting the mean and dividing by the standard deviation.

One way of achieving this is to normalize the rewards from every episode indpedendantly of other episodes. This gives better result than the naive approach. Another method is to train another agent to be the baseline, preferably using a simpler learning technique, and use the performance of that agent to approximate the mean of the rewards.


\section{Other methods}

\subsection{Monte Carlo Tree Search}

\subsection{Advantage Estimation}


\section{Gröbner bases}

Since the problem we're considering id the construction of Gröbner bases, let's give a short introduction to Gröbner bases.

We fix a field $k$ and consider the polynomial ring $R = k[x_{1}, \dots, x_{n}]$. For at set of polynomials $F = \{f_{1}, \dots, f_{l}\}$ we consider the ideal $I = \tuple{f_{1}, \dots, f_{l}}$ generated by these polynomials. Now, we wish to efficiently determine whether a given polynomial $f$ lies in $I$ or not. We know that $R$ is a \emph{unique factorization domain} so it is decidable, but giving an efficient algorithm is tricky. Instead, we extend the generating set of the ideal in a way that doesn't change the ideal but gives us stronger properties, enabling an efficient desicion algorithm.

Fix a \emph{term order} which is a well-order relation $>$ on $\mathbb N^{n}$ such that $a > b$ implies $a + c > b + c$ for any $a, b, c \in \mathbb N^{n}$. This naturally extends to a so called \emph{monomial order} on monomials $\{x^{v} = x_{1}^{v_{1}} \cdots x_{n}^{v_{n}} \mid v \in \mathbb N^{n}\}$ by comparing exponent vectors of the monomials. We'll write $>$ for both orders.

There are two common term orders: lexicographic and grevlex. Lexicographic has $a > b$ if there exists $k$ s.t. $a_{i} = b_{i}$ for $i < k$ and $a_{k} > b_{k}$. This is the usual ``alphabetix order'' on tuples. Grevlex ordering is often used in implementations using Gröbner bases and has $a > b$ if $\sum_{i} a_{i} > \sum_{j} b_{j}$ or   $\sum_{i} a_{i} = \sum_{j} b_{j}$ and the last non-zero entry of $a - b$ is negative. 

Given a monomial order we can define the \emph{initial term} and the S-polynomial.

\begin{definition}
  Given a monomial order $>$, the \emph{initial term} of a polynomial $f = \sum_{v} \lambda_{v} x^{v}$, denoted $in_{>}(f)$ is the greatest term of $f$ with respect to $>$. We will often omit the subscript when the order is either clear from context or arbitrary.
\end{definition}


Now, a naive division algorithm would look like this:

\begin{algorithm}[H]
\DontPrintSemicolon

  \KwInput{Polynomial $f$ and $F = \{f_{1}, \dots, f_{l}\}$}
  \KwOutput{Remainder $r$ s.t. $f - r \in \tuple F$ and $in(f_{i}) \nmid in(r)$ for all $f_{i} \in F$}
  $r \leftarrow f$\;
  \While{$\exists i. in(f_{i}) \mid in(r)$}
    {
        $r \leftarrow r - \frac{in(r)}{in(f_{i})} f_{i}$
    }

\caption{Division algorithm $reduce(f, F)$}
\end{algorithm}

This algorithm terminates since the inistal term of $r$ is strictly descreasing with respect to $>$.

Now, a Gröbner basis for an ideal $I$ is a set of polynomials $F = \{f_{1}, \dots, f_{l}\} \subseteq$ such that $in(f_{i}) \mid in(f)$ for all $f \in I\setminus \{0\}$.

\begin{theorem}
  Let $G = \{f_{1}, \dots, f_{l}\}$ be a Gröbner basis for an ideal $I$. Then $reduce(f, G) = 0 \Longleftrightarrow f \in I$.
\end{theorem}
\begin{proof}
  If $reduce(f, G) = 0$ then $f = f - 0 \in I$.

  If $f \in I$ and $f - r = h \in I$ then $r = f - h \in I$. But as guaranteed by the division algorithm, there is no $f_{i} \in G$ where $in(f_{i}) \mid in(r)$ even though $G$ is a Gröbner basis. The only element in $I$ not subject to the Gröbner basis constraint is zero, thus $r$ must be zero.
\end{proof}

An example where this does not hold without a Gröbner basis is example 5.4.3 in NL.

So, Gröbner bases are great, but how do we construct them? Do they even exist for every ideal? They do exist and we have an algorithm called Buchbergers algorithm to find them, bt first we need a construction called the S-polynomial or syzygy polynomial. 

\begin{definition}
  The S-polynomial of two polynomials $f$ and $g$ is denoted $S(f, g) = \frac{x^{w}}{in(f)} f - \frac{x^{w}}{in(g)} g$ where $x^{w} = lcm(in(f), in(g))$ is a least common multiple of $f$ and $g$.
\end{definition}

The Buchberger criterion is a simple was to check if we have a Gröbner basis, and it even leads us to an algorithm for constructing Gröbner bases.

\begin{theorem}\label{thm:buchberger_crit}
Let $F = \{f_{1}, \dots, f_{l}\}$ be a set of polynomials and let $I = \tuple F$ be the ideal generated by $F$. If $\text{reduce}(S(f_{i}, f_{j}), F) = 0$ for all $f_{i}, f_{j} \in F$ then $F$ is a Gröbner basis for $I$.
\end{theorem}
\begin{proof}
  See appendix.
\end{proof}

We can now present the Buchberger algorithm:

\begin{algorithm}[H]
\DontPrintSemicolon

  \KwInput{A set of polynomials $F = \{f_{1}, \dots, f_{l}\}$}
  \KwOutput{A Gröbner basis $G$ of the ideal $I = \tuple F$}
  $G \leftarrow F$ \;
  $P \leftarrow \{(f_{i}, f_{j}) \mid 1 \leq i < j \leq l\}$ \;
  \While{$|P| > 0$}
  {
    $(g, h) \leftarrow \text{select}(P)$ \;
    $P \leftarrow P \setminus \{(g, h)\}$ \;
    $r \leftarrow \text{reduce}(S(g, h), G)$ \;
    \If{$r \neq 0$}
    {
      $P \leftarrow \text{update}(P, G, r)$ \;
      $G \leftarrow G \cup \{r\}$ \;
    }
  }

\caption{Buchbergers algorithm}
\end{algorithm}

Notice that the algorithm uses 2 routines we haven't defined: \emph{select} and \emph{update}. The simplest implementations are $\text{select( $P$ )} = P_{1}$ taking the first pair in $P$ and $\text{update( $P, G, r$ )} = P \cup \{(f, r) \mid f \in G\}$.

Buchbergers algorithm adds remainders of syzygy polynomials until they all reduce to 0. Keeping theorem \ref{thm:buchberger_crit} in mind, it is clear that $G$ will be a Gröbner basis when the algorithm terminates. \colorbox{red}{TODO: dicksons lemma giver at buchberger terminerer.}

To produce a faster version of Buchbergers algorithm, these two routines are good places to start. During \emph{update} we can eliminate a large number of pairs using simpler criteria than theorem \ref{thm:buchberger_crit}. We'll discuss a better version in section \ref{sec:update}.

The problem of selecting the next pair of polynomials is not easy and it turns out to have serious consequences. Simply switching from taking the first pair to taking the last one, i.e. treating $P$ as a stack instead of a queue, can reduce the number of polynomial additions by almost 50\%.

There are a number of standard selection strategies:

\begin{itemize}
  \item[Random] Pick a random pair uniformly.

  \item[First] Pick the lexicographically smallest pair, where the order of polynomials is given by their order in $G$.

  \item[Degree] Pick the pair $(f, g)$ with the smallest total degree of $lcm(in(f), in(g))$.

  \item[Normal] Pick the pair $(f, g)$ where $lcm(in(f), in(g))$ is smallest in the used monomial order.
\end{itemize}


\subsection{A better \emph{update}}  \label{sec:update}
In the Buchberger algorithm, a pair $(f, g)$ will be removed if $\text{reduce}(S(f, g), G) = 0$. However, this is computationally expensive test. Fortunately, there are some easier tests that can eliminate a large number of pairs. I'll describe the tests described in Gebauer \& Möller.

When adding a reduced S-polynomial $r$ to the basis, i.e. when calling $\text{update}(P, G, r)$, consider every pair $(f, g) \in P$ and denote $\gamma = lcm(i(f), i(g))$. Then we can remove $(f, g)$ from $P$ if $\i(r) \mid \gamma$ and $\gamma \neq lcm(i(f), i(r))$ and $gamma \neq lcm(i(g), i(r))$. Indeed, in that case we would have $lcm(i(f), i(r)) \mid \gamma$ and $lcm(i(g), i(r)) \mid \gamma$. Thus writing $\gamma = q_{f} lcm(i(f), i(r)) = q_{g} lcm(i(g), i(r))$ we get that $S(f, g) = q_{f}S(f, r) - q_{g} S(g, r)$. This means $S(f, g)$ is redundant in a basis containing $S(f, r)$ and $S(g, r)$.

Next, we should add all pairs $(f, r)$ where $f \in G$. However, also here we can skip some immediately. First of all, for any two pairs $(f, r)$ and $(g, r)$ where $lcm(i(f), i(r)) \mid lcm(i(g), i(r))$ we can obviously remove $(g, r)$.

\end{document}


% \printbibliography

\end{document}
